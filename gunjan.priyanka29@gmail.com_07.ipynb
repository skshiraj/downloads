{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_Regression_using_SGD_(1) (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2S-uFqwSvmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUxLkBjISvmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = make_classification(n_samples=50000, n_features=15, n_informative=10, n_redundant=5,\n",
        "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xexp5GYNSvmz",
        "colab_type": "code",
        "outputId": "60bde65b-456a-4ebd-ce09-f41e2bb19b40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 15), (50000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54vJVc_KSvm9",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pKAn1-ASvm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r97pFTgrSvnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jykLIXZNSvnJ",
        "colab_type": "code",
        "outputId": "024b1dc2-5c2d-4ac0-cd66-96c41b7bb0b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((37500, 15), (37500,), (12500, 15), (12500,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0-M6oXASvnO",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sShoMeocSvnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import linear_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm6wi8L2SvnU",
        "colab_type": "code",
        "outputId": "942f38ba-67fe-4314-f087-f419a2cb7a0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# alpha : float\n",
        "# Constant that multiplies the regularization term. \n",
        "\n",
        "# eta0 : double\n",
        "# The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules.\n",
        "\n",
        "clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
        "clf"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
              "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
              "              fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
              "              loss='log', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
              "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
              "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4WFoxgASvnc",
        "colab_type": "code",
        "outputId": "6c14dec4-ff4e-43f9-9ca8-3224ca9f9554",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "clf.fit(X=X_train, y=y_train)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- Epoch 1\n",
            "Norm: 0.77, NNZs: 15, Bias: -0.316653, T: 37500, Avg. loss: 0.455552\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.91, NNZs: 15, Bias: -0.472747, T: 75000, Avg. loss: 0.394686\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.98, NNZs: 15, Bias: -0.580082, T: 112500, Avg. loss: 0.385711\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 1.02, NNZs: 15, Bias: -0.658292, T: 150000, Avg. loss: 0.382083\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 1.04, NNZs: 15, Bias: -0.719528, T: 187500, Avg. loss: 0.380486\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 1.05, NNZs: 15, Bias: -0.763409, T: 225000, Avg. loss: 0.379578\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 1.06, NNZs: 15, Bias: -0.795106, T: 262500, Avg. loss: 0.379150\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 1.06, NNZs: 15, Bias: -0.819925, T: 300000, Avg. loss: 0.378856\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 1.07, NNZs: 15, Bias: -0.837805, T: 337500, Avg. loss: 0.378585\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 1.08, NNZs: 15, Bias: -0.853138, T: 375000, Avg. loss: 0.378630\n",
            "Total training time: 0.13 seconds.\n",
            "Convergence after 10 epochs took 0.13 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
              "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
              "              fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
              "              loss='log', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
              "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
              "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WaVxhGpSvnj",
        "colab_type": "code",
        "outputId": "6525014c-186a-4085-f60a-de22ff0f61a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "clf.coef_, clf.coef_.shape, clf.intercept_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-0.42336692,  0.18547565, -0.14859036,  0.34144407, -0.2081867 ,\n",
              "          0.56016579, -0.45242483, -0.09408813,  0.2092732 ,  0.18084126,\n",
              "          0.19705191,  0.00421916, -0.0796037 ,  0.33852802,  0.02266721]]),\n",
              " (1, 15),\n",
              " array([-0.8531383]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su9e8fRLSvno",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcz5_UqCSvnq",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOBvEchCSvnr",
        "colab_type": "text"
      },
      "source": [
        "## Implement Logistc Regression with L2 regularization Using SGD: without using sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xbn61rrXSvnt",
        "colab_type": "text"
      },
      "source": [
        "### Instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14bA5yR3Svnv",
        "colab_type": "text"
      },
      "source": [
        "- Load the datasets(train and test) into the respective arrays"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7183hFBSvnv",
        "colab_type": "text"
      },
      "source": [
        "- Initialize the weight_vector and intercept term randomly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdLeFU0USvnx",
        "colab_type": "text"
      },
      "source": [
        "- Calculate the initlal log loss for the train and test data with the current weight and intercept and store it in a list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEVtAlO1Svny",
        "colab_type": "text"
      },
      "source": [
        "- for each epoch:\n",
        "    - for each batch of data points in train: (keep batch size=1)\n",
        "        - calculate the gradient of loss function w.r.t each weight in weight vector\n",
        "        - Calculate the gradient of the intercept <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>check this</a>\n",
        "        - Update weights and intercept (check the equation number 32 in the above mentioned <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>pdf</a>): <br>\n",
        "        $w^{(t+1)} ← (1 − \\frac{αλ}{N} )w^{(t)} + αx_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))$ <br>\n",
        "        $b^{(t+1)} ← (b^t +  α(y_n - σ((w^{(t)})^{T} x_n+b^{t}))$ \n",
        "        - calculate the log loss for train and test with the updated weights (you can check the python assignment 10th question)\n",
        "        - And if you wish, you can compare the previous loss and the current loss, if it is not updating, then\n",
        "        you can stop the training\n",
        "        - append this loss in the list ( this will be used to see how loss is changing for each epoch after the training is over )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qmRH4UpSvny",
        "colab_type": "text"
      },
      "source": [
        "- Plot the train and test loss i.e on x-axis the epoch number, and on y-axis the loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbZf9p5gSvn1",
        "colab_type": "text"
      },
      "source": [
        "- <strong>GOAL</strong>: compare your implementation and SGDClassifier's the weights and intercept, make sure they are as close as possible i.e difference should be in terms of 10^-3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fpz8X5DMSvn2",
        "colab_type": "code",
        "outputId": "514944ab-fcd2-435a-b2f5-8bbf118f816a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "w = np.zeros_like(X_train[0])\n",
        "b = 0\n",
        "eta0  = 0.0001\n",
        "alpha = 0.0001\n",
        "N = len(X_train)\n",
        "\n",
        "arr_loss=[]\n",
        "rounded_loss=[]\n",
        "\n",
        "#rounded_loss.append(0.0000)\n",
        "sigma=0\n",
        "w_at_the_end_of_each_epoch=[]\n",
        "b_at_the_end_of_each_epoch=[]\n",
        "\n",
        "def loss_function(y_train,y_pred):\n",
        "  p=-np.mean(y_train*np.log(y_pred)+(1-y_train)*np.log(1-y_pred))\n",
        "  return p\n",
        "\n",
        "\n",
        "for j in range(0,30): #for each epoch (30 iterations)\n",
        "  y_pred=[]\n",
        "  for i in range(len(X_train)): #for each point in train data\n",
        "    \n",
        "    z=(X_train[i].dot(w))+b\n",
        "    sigma=1/(1+np.exp(-z)) #sigmoid function\n",
        "    pred=sigma\n",
        "    w=(1-(eta0*alpha)/N)*w+(eta0*X_train[i]*(y_train[i]-sigma)) #updting weight vector\n",
        "    \n",
        "    b=(b+(eta0*(y_train[i]-sigma))) #updating the intercept\n",
        "\n",
        "    loss=loss_function(y_train[i],pred)\n",
        "    y_pred.append(loss)\n",
        "   \n",
        "  rounded_loss.append(round(np.average(y_pred),5))\n",
        "  w_at_the_end_of_each_epoch.append(w)\n",
        "  b_at_the_end_of_each_epoch.append(b)\n",
        "  arr_loss.append(np.average(y_pred))\n",
        "  if(j>0):\n",
        "    \n",
        "    if(rounded_loss[j]==rounded_loss[j-1]):\n",
        "      print(\"The number of iterations needed to find the same log loss for next epoch is:\")\n",
        "      print(j)\n",
        "      break\n",
        "\n",
        "print(\"the values for log loss at each epoch is :\")\n",
        "print(arr_loss)\n",
        "                  \n",
        "                  \n",
        "                  "
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of iterations needed to find the same log loss for next epoch is:\n",
            "13\n",
            "the values for log loss at each epoch is :\n",
            "[0.45610715612220937, 0.3946913054134686, 0.38558684615141536, 0.3820332220716334, 0.38035055342868135, 0.3794887755291076, 0.3790295733085725, 0.3787793587342117, 0.3786411532813051, 0.3785641439189919, 0.37852098163708703, 0.3784966943227099, 0.37848299250575157, 0.3784752507008752]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxW3jGjyCfRA",
        "colab_type": "code",
        "outputId": "6d969e75-4486-42d6-c471-ee7622a00dcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(\"the optimal w for the train data is :\")\n",
        "print(w)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the optimal w for the train data is :\n",
            "[-4.27659751e-01  1.92371836e-01 -1.47659040e-01  3.38112965e-01\n",
            " -2.18314654e-01  5.68456193e-01 -4.45219270e-01 -9.04783153e-02\n",
            "  2.20587410e-01  1.72532946e-01  1.97620190e-01  3.38326141e-04\n",
            " -8.02083343e-02  3.38994084e-01  2.27134519e-02]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6Y5kVscSvn5",
        "colab_type": "code",
        "outputId": "19b0a229-f5e3-4379-a23f-b9b70e5588a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "# write your code to implement SGD as per the above instructions\n",
        "# please choose the number of iternations on your own\n",
        "\n",
        "print(\"the optimal intercept for the train data is :\")\n",
        "print(b)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the optimal intercept for the train data is :\n",
            "-0.878773899259335\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNZ2JV_QH16p",
        "colab_type": "code",
        "outputId": "f2329af3-2f72-4bb9-9a3c-c4bf863b85d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "\n",
        "eta01  = 0.0001\n",
        "alpha1 = 0.0001\n",
        "N = len(X_test)\n",
        "arr_loss_test=[]\n",
        "\n",
        "sigma1=0\n",
        "\n",
        "\n",
        "def loss_function_test(y_test,y_pred):\n",
        "  p1=-np.mean(y_test*np.log(y_pred)+(1-y_test)*np.log(1-y_pred))\n",
        "\n",
        "  return p1\n",
        "\n",
        "for j in range(0,14): #for each epoch (13 iterations)\n",
        "  y_pred1=[]\n",
        "  for i in range(len(X_test)):\n",
        "    \n",
        "    z=(X_test[i].dot(w_at_the_end_of_each_epoch[j]))+b_at_the_end_of_each_epoch[j]\n",
        "    sigma1=1/(1+np.exp(-z))#sigmoid function\n",
        "    pred1=sigma1\n",
        "\n",
        "    loss1=loss_function_test(y_test[i],pred1)\n",
        "    y_pred1.append(loss1)\n",
        "   \n",
        "    \n",
        "  arr_loss_test.append(np.average(y_pred1))\n",
        "\n",
        "print(\"log loss for the test data for each epoch using w and b of the train data at the end of each epoch:\")\n",
        "print(arr_loss_test)#final array of loss for each epoch\n",
        "\n",
        "                  \n",
        "                  \n",
        "                  "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log loss for the test data for each epoch using w and b of the train data at the end of each epoch:\n",
            "[0.4051507711105075, 0.3900563342346352, 0.3850058403139176, 0.38272919624702656, 0.38158346823458406, 0.3809755312339999, 0.3806434506916482, 0.38045882555929145, 0.38035495698812244, 0.38029600513158396, 0.38026230396321115, 0.3802429117449629, 0.38023168086403236, 0.3802251317350343]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48gx6wQKSvoE",
        "colab_type": "code",
        "outputId": "9ca3cbb3-7140-4afb-9ab2-499c7cd48f5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure()\n",
        "label=[\"train loss\",\"test loss\"]\n",
        "plt.plot(np.arange(0,14), arr_loss[0:14])\n",
        "plt.plot(np.arange(0,14), arr_loss_test[0:14])\n",
        "plt.legend(label)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss at each epoch\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBCEiIjEpQYiISEz/H6561KNkk29uAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIk35bkULWdh",
        "colab_type": "code",
        "outputId": "a850dd12-a049-474e-c7f1-8e2478c8f07e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "\n",
        "count=0\n",
        "print(\"the differance between sklearn coeficient and implemented one is :\")\n",
        "for i in clf.coef_:\n",
        "  for y in i:\n",
        "    print(y-w[count])\n",
        "    count=count+1\n",
        "    \n",
        "   "
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the differance between sklearn coeficient and implemented one is :\n",
            "0.004292835029638842\n",
            "-0.006896183813516199\n",
            "-0.0009313171203083059\n",
            "0.0033311044167980697\n",
            "0.010127951882034664\n",
            "-0.008290407341635442\n",
            "-0.007205554929896996\n",
            "-0.003609812786818281\n",
            "-0.011314211155125542\n",
            "0.008308314397246996\n",
            "-0.000568284980862116\n",
            "0.0038808309367270345\n",
            "0.0006046383877040956\n",
            "-0.0004660685286295707\n",
            "-4.624238439203052e-05\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}